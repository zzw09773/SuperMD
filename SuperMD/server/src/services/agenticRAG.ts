import { ChatOpenAI } from '@langchain/openai';
import { StateGraph, Annotation, END } from '@langchain/langgraph';
import { BaseMessage, HumanMessage, AIMessage, SystemMessage } from '@langchain/core/messages';
import { DynamicTool } from '@langchain/core/tools';
import { searchSimilarDocuments } from './ragService';
import {
  appendAgentMemory,
  collectConversationContext,
  loadAgentMemory,
  memoryToMessages,
  type AgentMemoryEntryInput,
} from '../lib/agentMemory';
import { requireLLMConfig } from '../config/aiConfig';
import dotenv from 'dotenv';

dotenv.config();

const RAG_MEMORY_MODE = 'rag';

async function buildRAGMemory(userId: string) {
  const memory = await loadAgentMemory(userId, RAG_MEMORY_MODE);
  return {
    memory,
    context: collectConversationContext(memory),
    messages: memoryToMessages(memory, 'RAG å°è©±æ‘˜è¦'),
  };
}

// ============================================================
// 1ï¸âƒ£ STATE: å®šç¾© Agent ç‹€æ…‹
// ============================================================

/**
 * AgentState å®šç¾© Agentic RAG çš„ç‹€æ…‹çµæ§‹
 * - messages: å°è©±æ­·å²ï¼ˆç´¯åŠ ï¼‰
 * - query: åŸå§‹ç”¨æˆ¶æŸ¥è©¢
 * - rewrittenQuery: æ”¹å¯«å¾Œçš„æŸ¥è©¢ï¼ˆå¯é¸ï¼‰
 * - ragResults: æª¢ç´¢çµæœ
 * - shouldRetrieve: æ˜¯å¦éœ€è¦æª¢ç´¢
 * - answer: æœ€çµ‚ç­”æ¡ˆ
 * - steps: æ¨ç†æ­¥é©Ÿï¼ˆç”¨æ–¼ SSE ä¸²æµï¼‰
 */
const AgentState = Annotation.Root({
  messages: Annotation<BaseMessage[]>({
    reducer: (current, update) => current.concat(update),
    default: () => [],
  }),
  query: Annotation<string>({
    reducer: (_, update) => update,
    default: () => '',
  }),
  rewrittenQuery: Annotation<string | null>({
    reducer: (_, update) => update,
    default: () => null,
  }),
  ragResults: Annotation<string>({
    reducer: (_, update) => update,
    default: () => '',
  }),
  shouldRetrieve: Annotation<boolean>({
    reducer: (_, update) => update,
    default: () => true,
  }),
  answer: Annotation<string>({
    reducer: (_, update) => update,
    default: () => '',
  }),
  steps: Annotation<Array<{ type: string; content: string }>>({
    reducer: (current, update) => current.concat(update),
    default: () => [],
  }),
  userId: Annotation<string>({
    reducer: (_, update) => update,
    default: () => '',
  }),
});

type AgentStateType = typeof AgentState.State;

// ============================================================
// 2ï¸âƒ£ TOOLS: å®šç¾© Agent å¯ç”¨çš„å·¥å…·
// ============================================================

/**
 * Tool 1: Query Rewriter
 * å°‡ç”¨æˆ¶æŸ¥è©¢æ”¹å¯«ç‚ºæ›´é©åˆå‘é‡æª¢ç´¢çš„å½¢å¼
 */
const createQueryRewriterTool = (
  llm: ChatOpenAI,
  getContext: () => Promise<{ lastQuery?: string; lastAnswer?: string; lastSources?: string[] }>
) => {
  return new DynamicTool({
    name: 'query_rewriter',
    description: `Rewrites user queries to be more suitable for vector similarity search.
    Use this when the original query is too vague, conversational, or needs clarification.
    Input: original query string
    Output: rewritten query optimized for retrieval`,
    func: async (query: string) => {
      try {
        console.log(`?? [Query Rewriter] Original: "${query}"`);

        const context = await getContext();
        const contextLines: string[] = [];

        if (context.lastQuery) {
          contextLines.push(`Previous question: ${context.lastQuery}`);
        }

        if (context.lastAnswer) {
          contextLines.push(`Previous answer: ${context.lastAnswer}`);
        }

        if (context.lastSources && context.lastSources.length > 0) {
          contextLines.push(`Referenced documents: ${context.lastSources.join(', ')}`);
        }

        const contextBlock = contextLines.length > 0
          ? `Conversation context cannot be ignored. Latest information:
${contextLines.join('\n')}

`
          : '';

        const prompt = `${contextBlock}You are a query optimization expert. Rewrite the following user query so it works well for semantic search across the user's documents.

Original Query: "${query}"

Rules:
1. Resolve pronouns or vague references (e.g., "å‰›å‰›", "é‚£å€‹æ–‡ä»¶", "it") using the conversation context when provided.
2. Extract key concepts and keywords.
3. Remove conversational fluff (e.g., "can you help me", "I want to know").
4. Make it specific and focused.
5. Keep it concise (1-2 sentences) and in the same language as the original query.

Rewritten Query:`;

        const response = await llm.invoke([new HumanMessage(prompt)]);
        const rewrittenQuery = response.content.toString().trim();

        console.log(`? [Query Rewriter] Rewritten: "${rewrittenQuery}"`);
        return rewrittenQuery;
      } catch (error) {
        console.error('[Query Rewriter] Error:', error);
        return query; // Fallback to original
      }
    },
  });
};

/**
 * Tool 2: Knowledge Base Search
 * å¾ç”¨æˆ¶çš„çŸ¥è­˜åº«ä¸­æª¢ç´¢ç›¸é—œæ–‡ä»¶
 */
const createKnowledgeBaseSearchTool = (userId: string) => {
  return new DynamicTool({
    name: 'knowledge_base_search',
    description: `Searches the user's knowledge base (uploaded documents) for relevant information.
    Input: search query string (use rewritten query if available)
    Output: relevant text chunks with similarity scores and source files`,
    func: async (query: string) => {
      try {
        console.log(`ğŸ” [KB Search] Searching: "${query}"`);

        const results = await searchSimilarDocuments(query, userId, 5);

        if (results.length === 0) {
          return 'No relevant information found in the knowledge base. The user may need to upload relevant documents first.';
        }

        // Format results with source attribution
        const formattedResults = results
          .map((result, idx) => {
            return `[Source ${idx + 1}: ${result.fileName} | Similarity: ${(result.similarity * 100).toFixed(1)}%]
${result.content}
---`;
          })
          .join('\n');

        console.log(`âœ… [KB Search] Found ${results.length} relevant chunks`);
        return formattedResults;
      } catch (error) {
        console.error('[KB Search] Error:', error);
        return 'Error searching knowledge base: ' + (error instanceof Error ? error.message : 'Unknown error');
      }
    },
  });
};

/**
 * Tool 3: Relevance Evaluator
 * è©•ä¼°æª¢ç´¢çµæœæ˜¯å¦è¶³å¤ å›ç­”å•é¡Œ
 */
const createRelevanceEvaluatorTool = (llm: ChatOpenAI) => {
  return new DynamicTool({
    name: 'relevance_evaluator',
    description: `Evaluates if the retrieved documents contain sufficient information to answer the user's question.
    Input: JSON string with {query, results}
    Output: "SUFFICIENT" or "INSUFFICIENT" with reasoning`,
    func: async (input: string) => {
      try {
        const { query, results } = JSON.parse(input);
        console.log(`ğŸ“Š [Relevance Eval] Evaluating results for: "${query}"`);

        const prompt = `You are a relevance evaluator. Determine if the retrieved documents contain sufficient information to answer the user's question.

User Question: "${query}"

Retrieved Documents:
${results}

Evaluate:
1. Do the documents contain relevant information?
2. Is the information sufficient to provide a complete answer?
3. Are there any gaps in the information?

Respond with ONLY one word: "SUFFICIENT" or "INSUFFICIENT"
Then provide a brief 1-sentence explanation.

Format:
SUFFICIENT: [reason] OR INSUFFICIENT: [reason]`;

        const response = await llm.invoke([new HumanMessage(prompt)]);
        const evaluation = response.content.toString().trim();

        console.log(`ğŸ“Š [Relevance Eval] Result: ${evaluation}`);
        return evaluation;
      } catch (error) {
        console.error('[Relevance Eval] Error:', error);
        return 'SUFFICIENT: Proceeding with available information';
      }
    },
  });
};

// ============================================================
// 3ï¸âƒ£ NODES: å®šç¾© Graph çš„ç¯€é»ï¼ˆè™•ç†é‚è¼¯ï¼‰
// ============================================================

/**
 * Node 1: Router - æ±ºå®šæ˜¯å¦éœ€è¦æ”¹å¯«æŸ¥è©¢
 */
const routerNode = async (state: AgentStateType): Promise<Partial<AgentStateType>> => {
  console.log('\nğŸ”€ [Router Node] Analyzing query...');

  const { query } = state;

  // ç°¡å–®è¦å‰‡ï¼šå¦‚æœæŸ¥è©¢å¤ªçŸ­æˆ–å¤ªå£èªåŒ–ï¼Œå‰‡æ”¹å¯«
  const needsRewrite = query.length < 10 ||
                      query.toLowerCase().includes('è«‹å¹«æˆ‘') ||
                      query.toLowerCase().includes('can you') ||
                      query.toLowerCase().includes('æˆ‘æƒ³çŸ¥é“');

  const step = {
    type: 'reasoning',
    content: needsRewrite
      ? 'ğŸ¤” æŸ¥è©¢éœ€è¦å„ªåŒ–ä»¥æé«˜æª¢ç´¢æ•ˆæœ'
      : 'âœ… æŸ¥è©¢å·²è¶³å¤ æ˜ç¢ºï¼Œç›´æ¥æª¢ç´¢',
  };

  return {
    shouldRetrieve: needsRewrite,
    steps: [step],
  };
};

/**
 * Node 2: Query Rewriter - æ”¹å¯«æŸ¥è©¢
 */
const queryRewriterNode = async (
  state: AgentStateType,
  tools: { queryRewriter: DynamicTool }
): Promise<Partial<AgentStateType>> => {
  console.log('\nğŸ“ [Query Rewriter Node] Rewriting query...');

  const { query } = state;
  const rewrittenQuery = await tools.queryRewriter.func(query);

  const step = {
    type: 'tool_call',
    content: `ğŸ”„ æŸ¥è©¢æ”¹å¯«: "${query}" â†’ "${rewrittenQuery}"`,
  };

  return {
    rewrittenQuery,
    steps: [step],
  };
};

/**
 * Node 3: Retriever - åŸ·è¡ŒçŸ¥è­˜åº«æª¢ç´¢
 */
const retrieverNode = async (
  state: AgentStateType,
  tools: { kbSearch: DynamicTool }
): Promise<Partial<AgentStateType>> => {
  console.log('\nğŸ” [Retriever Node] Searching knowledge base...');

  const { query, rewrittenQuery } = state;
  const searchQuery = rewrittenQuery || query;

  const results = await tools.kbSearch.func(searchQuery);

  const step = {
    type: 'tool_call',
    content: `ğŸ” çŸ¥è­˜åº«æª¢ç´¢: ä½¿ç”¨æŸ¥è©¢ "${searchQuery}"`,
  };

  return {
    ragResults: results,
    steps: [step],
  };
};

/**
 * Node 4: Evaluator - è©•ä¼°æª¢ç´¢çµæœ
 */
const evaluatorNode = async (
  state: AgentStateType,
  tools: { relevanceEval: DynamicTool }
): Promise<Partial<AgentStateType>> => {
  console.log('\nğŸ“Š [Evaluator Node] Evaluating relevance...');

  const { query, ragResults } = state;

  const evaluation = await tools.relevanceEval.func(
    JSON.stringify({ query, results: ragResults })
  );

  const isSufficient = evaluation.toUpperCase().includes('SUFFICIENT');

  const step = {
    type: 'reasoning',
    content: `ğŸ“Š çµæœè©•ä¼°: ${evaluation}`,
  };

  return {
    shouldRetrieve: !isSufficient,
    steps: [step],
  };
};

/**
 * Node 5: Generator - ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ
 */
const generatorNode = async (
  state: AgentStateType,
  llm: ChatOpenAI
): Promise<Partial<AgentStateType>> => {
  console.log('\nğŸ’¬ [Generator Node] Generating answer...');

  const { query, ragResults } = state;

  const prompt = `You are a helpful AI assistant. Use the following information from the user's knowledge base to answer their question.

Knowledge Base Results:
${ragResults}

User Question: ${query}

Instructions:
1. Answer the question based ONLY on the information provided above
2. If the information is not sufficient, say so clearly
3. Always cite which source you used (e.g., "æ ¹æ“š Source 1...")
4. Be concise and accurate
5. Use Traditional Chinese (ç¹é«”ä¸­æ–‡) if the question is in Chinese, otherwise use English
6. Format your answer in Markdown for better readability

Answer:`;

  const response = await llm.invoke([new HumanMessage(prompt)]);
  const answer = response.content.toString();

  const step = {
    type: 'chunk',
    content: 'ğŸ’¬ æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...',
  };

  return {
    answer,
    steps: [step],
    messages: [new AIMessage(answer)],
  };
};

// ============================================================
// 4ï¸âƒ£ AGENT: çµ„è£ StateGraph å‰µå»º Agentic RAG
// ============================================================

/**
 * å‰µå»ºå®Œæ•´çš„ Agentic RAG Agent
 */
export const createAgenticRAG = (
  userId: string,
  contextLoader: () => Promise<{ lastQuery?: string; lastAnswer?: string; lastSources?: string[] }>
) => {
  // Initialize LLM
  const llmConfig = requireLLMConfig('AgenticRAG');
  const llm = new ChatOpenAI({
    modelName: llmConfig.modelName,
    temperature: 0.7,
    configuration: {
      apiKey: llmConfig.apiKey,
      baseURL: llmConfig.baseURL,
    },
  });

  // Create tools
  const tools = {
    queryRewriter: createQueryRewriterTool(llm, contextLoader),
    kbSearch: createKnowledgeBaseSearchTool(userId),
    relevanceEval: createRelevanceEvaluatorTool(llm),
  };

  // Build the graph
  const workflow = new StateGraph(AgentState)
    // Add nodes
    .addNode('router', routerNode)
    .addNode('rewriter', (state: AgentStateType) => queryRewriterNode(state, tools))
    .addNode('retriever', (state: AgentStateType) => retrieverNode(state, tools))
    .addNode('evaluator', (state: AgentStateType) => evaluatorNode(state, tools))
    .addNode('generator', (state: AgentStateType) => generatorNode(state, llm))

    // Define edges (workflow logic)
    .addEdge('__start__', 'router')
    .addConditionalEdges(
      'router',
      (state: AgentStateType) => {
        // å¦‚æœæŸ¥è©¢éœ€è¦æ”¹å¯«ï¼Œå» rewriterï¼Œå¦å‰‡ç›´æ¥æª¢ç´¢
        return state.shouldRetrieve ? 'rewriter' : 'retriever';
      },
      {
        rewriter: 'rewriter',
        retriever: 'retriever',
      }
    )
    .addEdge('rewriter', 'retriever')
    .addEdge('retriever', 'evaluator')
    .addConditionalEdges(
      'evaluator',
      (state: AgentStateType) => {
        // å¦‚æœçµæœä¸è¶³ï¼Œé‡æ–°æ”¹å¯«æŸ¥è©¢ï¼›å¦å‰‡ç”Ÿæˆç­”æ¡ˆ
        if (state.shouldRetrieve && state.rewrittenQuery) {
          // å·²ç¶“æ”¹å¯«éä¸€æ¬¡äº†ï¼Œç›´æ¥ç”Ÿæˆç­”æ¡ˆé¿å…ç„¡é™å¾ªç’°
          return 'generator';
        }
        return state.shouldRetrieve ? 'rewriter' : 'generator';
      },
      {
        rewriter: 'rewriter',
        generator: 'generator',
      }
    )
    .addEdge('generator', END);

  return workflow.compile();
};

/**
 * Query the Agentic RAG system
 */
export const queryAgenticRAG = async (
  userId: string,
  query: string
): Promise<{
  answer: string;
  sources: string[];
  steps: Array<{ type: string; content: string }>;
}> => {
  try {
    console.log(`\nğŸ¤– [Agentic RAG] Processing query: "${query}"`);
    console.log(`ğŸ‘¤ User ID: ${userId}`);

    const { messages: previousMessages, context } = await buildRAGMemory(userId);

    const agent = createAgenticRAG(userId, async () => context);

    const initialState = {
      query,
      userId,
      messages: [...previousMessages, new HumanMessage(query)],
      steps: [],
    };

    const finalState = await agent.invoke(initialState);

    // Extract sources from RAG results
    const sources: string[] = [];
    const sourceMatches = finalState.ragResults.match(/\[Source \d+: ([^\]]+?)\s*\|/g);
    if (sourceMatches) {
      sourceMatches.forEach((match: string) => {
        const fileName = match.match(/\[Source \d+: ([^\]]+?)\s*\|/)?.[1];
        if (fileName && !sources.includes(fileName)) {
          sources.push(fileName);
        }
      });
    }

    console.log(`âœ… [Agentic RAG] Query completed`);
    console.log(`ğŸ“ Steps taken: ${finalState.steps.length}`);
    console.log(`ğŸ“š Sources used: ${sources.length}`);
    const memoryEntries: AgentMemoryEntryInput[] = [
      { role: 'human' as const, content: query },
      {
        role: 'assistant' as const,
        content: finalState.answer,
        metadata: sources.length ? { sources } : undefined,
      },
    ];

    if (sources.length > 0) {
      memoryEntries.push({
        role: 'system' as const,
        content: `SOURCES: ${sources.join(', ')}`,
        metadata: { type: 'sources', sources },
      });
    }

    await appendAgentMemory(userId, RAG_MEMORY_MODE, memoryEntries);

    return {
      answer: finalState.answer,
      sources,
      steps: finalState.steps,
    };
  } catch (error) {
    console.error('âŒ [Agentic RAG] Query error:', error);
    throw error;
  }
};

/**
 * Streaming version for SSE support
 */
export async function* streamAgenticRAG(
  userId: string,
  query: string
): AsyncGenerator<{ type: string; content: string; done?: boolean; answer?: string; sources?: string[] }> {
  try {
    console.log(`\nğŸ¬ [Agentic RAG Stream] Starting: "${query}"`);

    const { messages: previousMessages, context } = await buildRAGMemory(userId);

    const agent = createAgenticRAG(userId, async () => context);

    const initialState = {
      query,
      userId,
      messages: [...previousMessages, new HumanMessage(query)],
      steps: [],
    };

    // Stream each step
    for await (const event of await agent.stream(initialState)) {
      const nodeName = Object.keys(event)[0];
      const nodeState = (event as Record<string, AgentStateType>)[nodeName];

      console.log(`ğŸ“¡ [Stream] Node: ${nodeName}`);

      // Stream steps as they happen
      if (nodeState.steps && nodeState.steps.length > 0) {
        for (const step of nodeState.steps) {
          yield {
            type: step.type,
            content: step.content,
          };
        }
      }
    }

    // Get final state
    const finalState = await agent.invoke(initialState);

    // Extract sources
    const sources: string[] = [];
    const sourceMatches = finalState.ragResults.match(/\[Source \d+: ([^\]]+?)\s*\|/g);
    if (sourceMatches) {
      sourceMatches.forEach((match: string) => {
        const fileName = match.match(/\[Source \d+: ([^\]]+?)\s*\|/)?.[1];
        if (fileName && !sources.includes(fileName)) {
          sources.push(fileName);
        }
      });
    }

    // Final result
    yield {
      type: 'done',
      content: finalState.answer,
      done: true,
      answer: finalState.answer,
      sources,
    };
    const memoryEntries: AgentMemoryEntryInput[] = [
      { role: 'human' as const, content: query },
      {
        role: 'assistant' as const,
        content: finalState.answer,
        metadata: sources.length ? { sources } : undefined,
      },
    ];

    if (sources.length > 0) {
      memoryEntries.push({
        role: 'system' as const,
        content: `SOURCES: ${sources.join(', ')}`,
        metadata: { type: 'sources', sources },
      });
    }

    await appendAgentMemory(userId, RAG_MEMORY_MODE, memoryEntries);

    console.log(`âœ… [Agentic RAG Stream] Completed`);
  } catch (error) {
    console.error('âŒ [Agentic RAG Stream] Error:', error);
    yield {
      type: 'error',
      content: error instanceof Error ? error.message : 'Unknown error',
      done: true,
    };
  }
}

